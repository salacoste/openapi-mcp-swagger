# Story 3.1: Core Search Infrastructure Setup

## Story Overview
**Epic**: Epic 3 - Search & Discovery Engine
**Story ID**: 3.1
**Story Type**: Core Infrastructure
**Complexity**: Medium-High
**Sprint**: TBD

## User Story
As a system,
I want to implement foundational search infrastructure with indexing capabilities,
So that API documentation can be efficiently searched and ranked by relevance.

## Story Context

### Business Value
- Provides intelligent search foundation that goes beyond simple keyword matching
- Enables contextual API discovery with relevance ranking for better user experience
- Delivers core value proposition: intelligent search across large API documentation
- Sets foundation for advanced search features like query suggestions and auto-completion

### Dependencies
**Depends On:**
- Story 1.3: Schema Normalization Engine (normalized API data structure)
- Story 1.4: Basic Storage Layer Implementation (queryable database with API data)
- Story 2.2: searchEndpoints MCP Method Implementation (search method framework)

**Enables:**
- Story 3.2: Intelligent Endpoint Indexing (comprehensive endpoint search documents)
- Story 3.3: Advanced Search Query Processing (sophisticated query handling)
- Story 3.4: Search Result Optimization and Filtering (enhanced result processing)

### Technical Context
**Existing System Integration:**
- Integrates with: Normalized API data from Epic 1.3, database layer from Epic 1.4
- Technology: Python with Whoosh library for pure-Python BM25 search, index management
- Follows pattern: Database query optimization patterns, configuration management from CLI
- Touch points: API endpoints, schemas, parameters data, search index files, configuration system

## Acceptance Criteria

### Functional Requirements
1. **BM25 Search Engine Implementation**
   - Implement Whoosh-based search engine with BM25 ranking algorithm
   - Create search index schema supporting endpoints, parameters, schemas, descriptions
   - Configure field weights for optimal ranking: paths (1.5x), descriptions (1.0x), parameters (0.8x)
   - Index initialization from existing normalized API data in database

2. **Search Index Structure Design**
   - Design index schema with fields: endpoint_path, http_method, description, parameters, tags
   - Support full-text indexing of descriptions and parameter documentation
   - Include metadata fields: endpoint_id, schema_references, authentication_requirements
   - Implement index versioning for schema evolution and data migration

3. **Index Creation Pipeline**
   - Process normalized API data from database into searchable documents
   - Extract searchable content from endpoints, parameters, descriptions, and schemas
   - Create composite search documents capturing complete endpoint context
   - Batch processing for large API documentation sets with progress tracking

### Integration Requirements
4. **Database Integration and Data Flow**
   - Use existing normalized data from Epic 1.3 without database schema changes
   - Query optimization for extracting search-indexable data from multiple tables
   - Maintain data consistency between database and search index
   - Integration with existing database connection pooling and transaction management

5. **Configuration and File Management**
   - Extend existing CLI configuration with search engine settings
   - Configurable index storage location and index optimization parameters
   - Index file management: creation, update, backup, and cleanup procedures
   - Environment-specific configuration for development vs. production indexing

6. **Performance and Resource Management**
   - Index creation optimized for sub-200ms search response times per NFR1
   - Memory-efficient indexing for large API documentation sets (10MB+ Swagger files)
   - Index size optimization: balance between search quality and storage requirements
   - Concurrent index access patterns supporting multiple search threads

### Quality Requirements
7. **Index Quality and Accuracy**
   - Complete indexing of all normalized API data without data loss
   - Search relevance testing with sample queries and expected results
   - Index integrity validation after creation and updates
   - Proper handling of special characters, Unicode, and technical terminology

8. **Incremental Index Updates**
   - Support for incremental index updates when API data changes
   - Delta indexing for modified endpoints without full index rebuild
   - Index update performance: minimize disruption to ongoing search operations
   - Consistency guarantees during index update operations

9. **Search Performance Foundation**
   - Index optimization for target search performance (<200ms response time)
   - Index size monitoring and optimization recommendations
   - Search index warming and caching strategies
   - Performance benchmarking tools for search index evaluation

## Technical Implementation Notes

### Search Index Architecture
```python
from whoosh.index import create_index
from whoosh.fields import Schema, TEXT, ID, KEYWORD
from whoosh.analysis import StemmingAnalyzer

# Search index schema definition
search_schema = Schema(
    endpoint_id=ID(stored=True, unique=True),
    endpoint_path=TEXT(stored=True, analyzer=StemmingAnalyzer(), field_boost=1.5),
    http_method=KEYWORD(stored=True),
    description=TEXT(stored=True, analyzer=StemmingAnalyzer()),
    parameters=TEXT(stored=True, analyzer=StemmingAnalyzer(), field_boost=0.8),
    tags=KEYWORD(stored=True),
    authentication=KEYWORD(stored=True),
    schema_refs=TEXT(stored=True)
)

class SearchIndexManager:
    """Manages search index creation, updates, and optimization"""

    def __init__(self, index_dir: str, database_connection):
        self.index_dir = index_dir
        self.db = database_connection
        self.index = None

    async def create_index_from_database(self):
        """Create search index from normalized database data"""
        # 1. Query normalized endpoint data
        # 2. Process into searchable documents
        # 3. Batch index creation with progress tracking
        # 4. Index optimization and verification
```

### Index Document Processing
```python
async def create_search_document(endpoint_data: Dict) -> Dict:
    """Convert database endpoint data to search document"""
    return {
        "endpoint_id": endpoint_data["id"],
        "endpoint_path": endpoint_data["path"],
        "http_method": endpoint_data["method"],
        "description": f"{endpoint_data['summary']} {endpoint_data['description']}",
        "parameters": extract_parameter_text(endpoint_data["parameters"]),
        "tags": " ".join(endpoint_data["tags"]),
        "authentication": extract_auth_requirements(endpoint_data["security"]),
        "schema_refs": extract_schema_references(endpoint_data["schemas"])
    }
```

### Configuration Integration
```yaml
# search engine section in existing config
search_engine:
  enabled: true
  engine_type: "whoosh"  # Pure Python BM25 implementation
  index_directory: "./search_index"
  field_weights:
    endpoint_path: 1.5
    description: 1.0
    parameters: 0.8
    tags: 0.6
  indexing:
    batch_size: 1000
    optimization_threshold: 10000  # docs before optimization
    incremental_updates: true
  performance:
    cache_size_mb: 64
    max_search_results: 1000
```

### Index Management Operations
- **Create**: Full index creation from database with progress tracking
- **Update**: Incremental updates for modified endpoints
- **Optimize**: Index optimization for search performance
- **Backup**: Index backup and restore capabilities
- **Validate**: Index integrity and completeness validation

## Definition of Done
- [x] Whoosh BM25 search engine implemented with proper configuration
- [x] Search index schema supports endpoints, parameters, descriptions, and metadata
- [x] Field weights configured for optimal ranking (paths 1.5x, descriptions 1.0x, parameters 0.8x)
- [x] Index creation pipeline processes all normalized API data from database
- [x] Batch processing handles large API documentation sets efficiently
- [x] Integration with existing database layer maintains data consistency
- [x] Configuration system extended with search engine settings
- [x] Index file management supports creation, update, backup, and cleanup
- [x] Performance optimization targets <200ms search response time foundation
- [x] Incremental index updates work without full rebuild
- [x] Index quality validation ensures complete and accurate data representation
- [x] Comprehensive unit tests for index creation and management (â‰¥80% coverage)
- [x] Integration tests with universal API data validate real-world indexing (not limited to Ozon)
- [x] Performance testing validates indexing speed and search response preparation
- [ ] Documentation updated with search configuration and index management procedures
- [x] Index integrity tools validate completeness and accuracy

## Validation Criteria
- Search index successfully created from all normalized API data
- Index schema supports all required search fields and metadata
- Field weights produce relevant search ranking for test queries
- Index creation completes within reasonable time for large API sets
- Incremental updates maintain index consistency with database changes
- Search index provides foundation for <200ms response time target

## Risk Assessment
**Medium-High Risk** - Search engine integration complexity and performance optimization

**Primary Risks:**
- Whoosh library performance scaling with large API documentation sets
- Index size growth impacting storage and memory requirements
- Complex field weighting balance affecting search relevance quality
- Index update consistency during concurrent database modifications

**Mitigation Strategies:**
- Start with smaller test APIs and incrementally scale to enterprise sizes
- Monitor index size and implement optimization strategies early
- A/B testing for field weight optimization with sample queries
- Database change detection and coordinated index update mechanisms
- Performance benchmarking throughout development process

**Rollback Plan:**
- Disable search indexing and fall back to simple database queries in searchEndpoints
- Remove search index files and revert to basic SQL LIKE pattern matching
- Existing MCP methods continue working with reduced search capabilities
- Database and core functionality remain completely unaffected

## Story Estimation
**Complexity**: Medium-High
**Effort**: 4-5 development sessions (16-20 hours)
**Risk Level**: Medium-High
**Dependencies**: 3 (Stories 1.3, 1.4, 2.2)

---

## Dev Agent Record

### Agent Model Used
Claude 3.5 Sonnet (claude-3-5-sonnet-20241022)

### Tasks
- [x] Create search module directory structure and base files
- [x] Implement SearchIndexManager class with Whoosh BM25 integration
- [x] Create search index schema with proper field weights and configuration
- [x] Implement index creation pipeline from database data
- [x] Add search configuration to settings system
- [x] Implement incremental index updates functionality
- [x] Create comprehensive unit tests for search infrastructure
- [x] Add integration tests with universal API data (not limited to Ozon)
- [x] Run performance validation and optimization

### Completion Notes
- Successfully implemented core search infrastructure using Whoosh BM25 search engine
- Created comprehensive search index schema with configurable field weights (endpoint_path: 1.5x, description: 1.0x, parameters: 0.8x)
- Built SearchIndexManager with full index lifecycle management (create, update, optimize, validate)
- Implemented SearchEngine with advanced search capabilities including filters, pagination, and suggestions
- Added RelevanceRanker with BM25 and custom scoring algorithms
- Extended configuration system with hierarchical search settings
- Created extensive unit tests covering all components with edge cases
- Built integration tests demonstrating universal API compatibility (not limited to any specific API)
- Validated performance requirements and schema functionality
- All core requirements from Story 3.1 completed successfully

### File List
**New Files Created:**
- `src/swagger_mcp_server/search/__init__.py` - Search module entry point
- `src/swagger_mcp_server/search/index_schema.py` - Search index schema definitions
- `src/swagger_mcp_server/search/index_manager.py` - Core index management functionality
- `src/swagger_mcp_server/search/search_engine.py` - High-level search interface
- `src/swagger_mcp_server/search/relevance.py` - Relevance ranking algorithms
- `src/tests/unit/test_search/__init__.py` - Unit test package
- `src/tests/unit/test_search/test_index_schema.py` - Schema tests
- `src/tests/unit/test_search/test_index_manager.py` - Index manager tests
- `src/tests/unit/test_search/test_search_engine.py` - Search engine tests
- `src/tests/unit/test_search/test_relevance.py` - Relevance ranking tests
- `src/tests/integration/test_search/__init__.py` - Integration test package
- `src/tests/integration/test_search/test_universal_api_indexing.py` - Universal API compatibility tests
- `src/tests/integration/test_search/test_real_world_scenarios.py` - Real-world usage scenario tests

**Modified Files:**
- `src/swagger_mcp_server/config/settings.py` - Extended with comprehensive search configuration

### Change Log
- **2024-01-XX**: Implemented complete search infrastructure for Story 3.1
- **Core Implementation**: Whoosh BM25 search engine with configurable field weights
- **Architecture**: Modular design with SearchIndexManager, SearchEngine, and RelevanceRanker
- **Configuration**: Hierarchical search settings integrated into existing config system
- **Testing**: Comprehensive unit and integration tests ensuring universal API compatibility
- **Performance**: Validated sub-second indexing and search response capabilities

### Status
Ready for Review